{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('torchkgae': conda)",
   "display_name": "Python 3.8.5 64-bit ('torchkgae': conda)",
   "metadata": {
    "interpreter": {
     "hash": "334c8f8e89452fc07a078fc466e9ca5cd061fa11063f9d04ce9b4f9d89d22362"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding Relational Graph Convolutional Networks (R-GCNs)\n",
    "What happens under the hood of Graph Neural Networks (GNNs) applied to multi-relational data, such as Knowledge Graphs (KGs)? A brief introduction to R-GCNs using pure numpy.\n",
    "\n",
    "Original Paper: Schlichtkrull, M., Kipf, T. N., Bloem, P., Van Den Berg, R., Titov, I., & Welling, M. (2018, June). *Modeling relational data with graph convolutional networks*. In *European Semantic Web Conference* (pp. 593-607). Springer, Cham. \n",
    "\n",
    "## Requirements\n",
    "- Numpy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "except ImportError as e:\n",
    "    print('numpy is not available in you environment: try \"pip install numpy\"')\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall on the GNNs (and the Vanilla GCNs)\n",
    "This section provides a recall on the behaviour of a basic GNN layer. \n",
    "\n",
    "Main ingredients:\n",
    "- One-hot vectors (no features) adopted to represent nodes.\n",
    "- Weight matrix representing the learnable parameters (or weights).\n",
    "- Adjacency matrix describing undirected edges between nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n----- One-hot vector representation of nodes:\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]]\n\n\n----- Weight Matrix:\n[[-0.4294049   0.57624235 -0.3047382 ]\n [-0.11941829 -0.12942953  0.19600584]\n [ 0.5029172   0.3998854  -0.21561317]\n [ 0.02834577 -0.06529497 -0.31225734]\n [ 0.03973776  0.47800217 -0.04941563]]\n\n\n----- Adjacency Matrix (undirected graph):\n[[1 1 1 0 1]\n [1 1 1 1 1]\n [1 1 1 1 0]\n [0 1 1 1 1]\n [1 1 0 1 1]]\n"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# One-hot vectors for representing nodes (randomly initialized)\n",
    "X = np.eye(5, 5)\n",
    "n = X.shape[0]\n",
    "np.random.shuffle(X)\n",
    "\n",
    "print('\\n\\n----- One-hot vector representation of nodes:')\n",
    "print(X)\n",
    "\n",
    "# Low-dimensional vector to represent node embeddings\n",
    "emb = 3 \n",
    "\n",
    "# Weight matrix (randomly inizialized according to Glorot and Bengio (2010))\n",
    "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (n, emb))\n",
    "\n",
    "print('\\n\\n----- Weight Matrix:')\n",
    "print(W)\n",
    "\n",
    "# Adjacency matrix (randomly initialized)\n",
    "A = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 1)  # Include the self loop\n",
    "A_und = (A + A.T) # Hack for creating a symmetric adjacency matrix (undirected graph)\n",
    "A_und[A_und > 1] = 1\n",
    "\n",
    "print('\\n\\n----- Adjacency Matrix (undirected graph):')\n",
    "print(A_und)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering these ingredients, a \"recursive neighborhood diffusion\" is performed through the so-called “message passing framework”. The main idea behind this framework is that each node representation is updated with its neighbors' features. The neighbors' features are *passed* to the target node as *messages* through the edges. \n",
    "\n",
    "The operations are the following:\n",
    "* A linear transformation (or projection) involving the nodes features and the weight matrix.\n",
    "* A neighborhood diffusion to update the nodes representations, aggregating the features of its neighbors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n----- Output of the linear transformation:\n[[ 0.5029172   0.3998854  -0.21561317]\n [-0.11941829 -0.12942953  0.19600584]\n [ 0.03973776  0.47800217 -0.04941563]\n [-0.4294049   0.57624235 -0.3047382 ]\n [ 0.02834577 -0.06529497 -0.31225734]]\n\n\n----- (GNN) Output of the neighborhood diffusion:\n[[ 0.45158244  0.68316307 -0.3812803 ]\n [ 0.02217754  1.25940542 -0.6860185 ]\n [-0.00616823  1.3247004  -0.37376116]\n [-0.48073966  0.85952002 -0.47040533]\n [-0.01756022  0.78140325 -0.63660287]]\n"
    }
   ],
   "source": [
    "# Linear transformation\n",
    "L_0 = X.dot(W)\n",
    "\n",
    "print('\\n\\n----- Output of the linear transformation:')\n",
    "print(L_0)\n",
    "\n",
    "# Neighborhood diffusion\n",
    "ND_GNN = A_und.dot(L_0)\n",
    "\n",
    "print('\\n\\n----- (GNN) Output of the neighborhood diffusion:')\n",
    "print(ND_GNN)\n",
    "\n",
    "# Test on the aggregation:\n",
    "assert(ND_GNN[0,0] == L_0[0,0] + L_0[1,0] + L_0[2,0] + L_0[4,0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest formulation of the GNN, represented by the Vanilla Graph Convolutional Networks (GCNs), the aggregation/update operation is an isotropic computation, where the features of neighbor nodes are considered in the same way. \n",
    "\n",
    "More precisely, an isotropic *average* computation is performed in the specific case of Vanilla GCNs. This average operation requires a new ingredient represented by the indegree of each node, which consists in the number of incoming edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n----- Degree vector - Each element represents the i-node degree:\n[4 5 4 4 4]\n\n\n----- Reciprocal of the degree (in a diagonal matrix):\n[[0.25 0.   0.   0.   0.  ]\n [0.   0.2  0.   0.   0.  ]\n [0.   0.   0.25 0.   0.  ]\n [0.   0.   0.   0.25 0.  ]\n [0.   0.   0.   0.   0.25]]\n\n\n----- (GCN) Isotropic average computation:\n[[ 0.11289561  0.17079077 -0.09532007]\n [ 0.00443551  0.25188109 -0.1372037 ]\n [-0.00154206  0.3311751  -0.09344029]\n [-0.12018491  0.21488001 -0.11760133]\n [-0.00439005  0.19535081 -0.15915072]]\n"
    }
   ],
   "source": [
    "# Degree vector (degree for each node)\n",
    "D = A_und.sum(axis=1)\n",
    "\n",
    "print('\\n\\n----- Degree vector - Each element represents the i-node degree:')\n",
    "print(D)\n",
    "\n",
    "# Reciprocal of the degree to perform the average computation (diagonal matrix)\n",
    "D_rec = np.diag(np.reciprocal(D.astype(np.float32))) # Need to convert degree integer values as float\n",
    "\n",
    "print('\\n\\n----- Reciprocal of the degree (in a diagonal matrix):')\n",
    "print(D_rec)\n",
    "\n",
    "# Isotropic average computation\n",
    "ND_GCN = D_rec.dot(ND_GNN)\n",
    "\n",
    "print('\\n\\n----- (GCN) Isotropic average computation:')\n",
    "print(ND_GCN)\n",
    "\n",
    "# Test on the isotropic average computation:\n",
    "assert(ND_GCN[0,0] == ND_GNN[0,0] * D_rec[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From GCNs to R-GCNs for encoding KGs\n",
    "The previous example considers an undirected and no-typed graph. As mentioned before, the update process is based on the following steps (the node indegree is not considered for the sake of simplicity):\n",
    "\n",
    "1. a projection step (or linear transformation), which is achieved multiplying of: (i) the one-hot feature matrix with (ii) the weight matrix.\n",
    "    \n",
    "    (i). Matrix (n, n) that defines the initial features of the nodes.\n",
    "    \n",
    "    (ii). Matrix (n, emb) that describes the model's parameters. The current matrix is able to encode only one type of relation.\n",
    "\n",
    "2. an aggregation step, which is achieved multiplying: (i) the adjacency matrix with (ii) the matrix resulting from the projection step.\n",
    "\n",
    "    (i). Symmetric Matrix (n, n) that describes undirected and untyped edges.\n",
    "\n",
    "    (ii). Matrix (n, emb) that describes the latent node representation of nodes.\n",
    "\n",
    "In order to extend the GCN layer to encode the structure of a KG, we need to represent our data as a directed and multi-typed graph. The update process is similar to the previous one, but the ingredients are more complex:\n",
    "\n",
    "1. a projection step, which is achieved multiplying: (i) the one-hot feature matrix with (ii) the weight **tensor**.\n",
    "    \n",
    "    (i). Matrix (n, n) that defines the initial features of the nodes.\n",
    "    \n",
    "    (ii). **Tensor (r, n, emb)** that describes the model's parameters, which will embed the latent node representations at the end of the training process. This tensor is able to encode different relations by stacking **r** batches of matrices (n, emb). Each of these batches encodes a single typed relation.\n",
    "\n",
    "Tip: the projection step will no longer be a simple multiplication of matrices, but it will be a *batch matrix multiplication*, in which (i) is multiplied with each batch of (ii).\n",
    "\n",
    "2. an aggregation step, which is achieved multiplying (i) the **(directed) adjacency tensor** with (ii) the **tensor** resulting from the projection step.\n",
    "\n",
    "    (i) **Tensor (r, n, n)** that describes directed and **r**-typed edges. This tensor is composed of **r** batches of adjacency matrices (n,n). In detail, each of these matrices describes the edges between nodes, according to a specific type of relation. Moreover, compared to the adjacency matrix of an undirected graph, each of these adjacency matrices is not symmetric, because it encodes a specific edge direction.\n",
    "    (ii) **Tensor (r, n, emb)** is the result of the projection layer.\n",
    "\n",
    "Tip: as happened for the projection step, the aggregation phase consists of a *batch matrix multiplication*. Each batch of (i) is multiplied with each batch of (ii). This aggregation defines the GCN transformation for each batch. At the end of the process, the batches have to be added together (R-GCN) to obtain a node representation that incorporates the neighborhood aggregation according to different type of relations.\n",
    "\n",
    "The following example shows the behaviour of a R-GCN layer encoding a directed and multi-typed graph with 2 types of edges (or relations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\n\n----- Recall --&gt; One-hot vector representation of nodes:\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]]\n\n\n----- Number of relation types:\n2\n\n\n----- Weight matrix of relation 1:\n[[-0.46378913 -0.09109707  0.52872529]\n [ 0.03829597  0.22156061 -0.2130242 ]\n [ 0.21535272  0.38639244 -0.55623279]\n [ 0.28884178  0.56448816  0.28655701]\n [-0.25352144  0.334031   -0.45815514]]\n\n\n----- Weight matrix of relation 2:\n[[0.22946783 0.4552118  0.15387093]\n [0.15100992 0.073714   0.01948981]\n [0.34262941 0.11369778 0.14011786]\n [0.25087085 0.03614765 0.29131763]\n [0.081897   0.29875971 0.3528816 ]]\n\n\n-----Tensor including both weight matrices:\n[[[-0.46378913 -0.09109707  0.52872529]\n  [ 0.03829597  0.22156061 -0.2130242 ]\n  [ 0.21535272  0.38639244 -0.55623279]\n  [ 0.28884178  0.56448816  0.28655701]\n  [-0.25352144  0.334031   -0.45815514]]\n\n [[ 0.22946783  0.4552118   0.15387093]\n  [ 0.15100992  0.073714    0.01948981]\n  [ 0.34262941  0.11369778  0.14011786]\n  [ 0.25087085  0.03614765  0.29131763]\n  [ 0.081897    0.29875971  0.3528816 ]]]\n\n\n----- Linear trasformation (or projection) with batch matrix multiplication:\n[[[ 0.21535272  0.38639244 -0.55623279]\n  [ 0.03829597  0.22156061 -0.2130242 ]\n  [-0.25352144  0.334031   -0.45815514]\n  [-0.46378913 -0.09109707  0.52872529]\n  [ 0.28884178  0.56448816  0.28655701]]\n\n [[ 0.34262941  0.11369778  0.14011786]\n  [ 0.15100992  0.073714    0.01948981]\n  [ 0.081897    0.29875971  0.3528816 ]\n  [ 0.22946783  0.4552118   0.15387093]\n  [ 0.25087085  0.03614765  0.29131763]]]\n\n\n----- Adjacency matrix of relation 1:\n[[0 1 1 1 1]\n [1 1 0 0 1]\n [1 0 0 1 0]\n [0 0 1 1 1]\n [1 1 0 1 0]]\n\n\n----- Adjacency matrix of relation 2:\n[[0 0 0 1 0]\n [1 0 0 0 0]\n [1 0 0 1 1]\n [0 0 0 0 0]\n [0 1 0 0 0]]\n\n\n----- Tensor including both adjacency matrices:\n[[[0 1 1 1 1]\n  [1 1 0 0 1]\n  [1 0 0 1 0]\n  [0 0 1 1 1]\n  [1 1 0 1 0]]\n\n [[0 0 0 1 0]\n  [1 0 0 0 0]\n  [1 0 0 1 1]\n  [0 0 0 0 0]\n  [0 1 0 0 0]]]\n\n\n----- (GCN) Output of the neighborhood diffusion (for each typed edge):\n[[[-0.39017282  1.0289827   0.14410296]\n  [ 0.54249047  1.17244121 -0.48269997]\n  [-0.24843641  0.29529538 -0.0275075 ]\n  [-0.42846879  0.80742209  0.35712716]\n  [-0.21014043  0.51685598 -0.2405317 ]]\n\n [[ 0.22946783  0.4552118   0.15387093]\n  [ 0.34262941  0.11369778  0.14011786]\n  [ 0.82296809  0.60505722  0.58530642]\n  [ 0.          0.          0.        ]\n  [ 0.15100992  0.073714    0.01948981]]]\n\n\n----- (R-GCN) Aggregation of the results of the GCN layer applied to different types of edge:\n[[-0.16070499  1.48419449  0.29797389]\n [ 0.88511988  1.28613899 -0.34258211]\n [ 0.57453168  0.9003526   0.55779892]\n [-0.42846879  0.80742209  0.35712716]\n [-0.05913052  0.59056998 -0.22104189]]\n"
    }
   ],
   "source": [
    "print('\\n\\n----- Recall --> One-hot vector representation of nodes:')\n",
    "print(X)\n",
    "\n",
    "# Number of relation types\n",
    "num_rels = 2\n",
    "\n",
    "print('\\n\\n----- Number of relation types:')\n",
    "print(num_rels)\n",
    "\n",
    "# Weight matrix of relation number 1 (randomly inizialized according to Glorot and Bengio (2010))\n",
    "W_rel1 = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (n, emb))\n",
    "print('\\n\\n----- Weight matrix of relation 1:')\n",
    "print(W_rel1)\n",
    "\n",
    "# Weight matrix of relation number 2 (randomly initialized with uniform distribution)\n",
    "W_rel2 = np.random.uniform(1/100, 0.5, (n, emb))\n",
    "print('\\n\\n----- Weight matrix of relation 2:')\n",
    "print(W_rel2)\n",
    "\n",
    "# Tensor including both weight matrices\n",
    "W_rels =  np.concatenate((W_rel1, W_rel2))\n",
    "W_rels = np.reshape(W_rels,(num_rels, n, emb)) # num_rels is the number of the relations, n is the number of nodes, emb is the low-dimensional representation\n",
    "print('\\n\\n-----Tensor including both weight matrices:')\n",
    "print(W_rels)\n",
    "\n",
    "L_0_rels = np.matmul(X, W_rels)\n",
    "print('\\n\\n----- Linear trasformation (or projection) with batch matrix multiplication:')\n",
    "print(L_0_rels)\n",
    "\n",
    "# Adjacency matrix of relation number 1\n",
    "A_rel1 = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 0)  # Not consider the self loop (diag values = 0)\n",
    "print('\\n\\n----- Adjacency matrix of relation 1:')\n",
    "print(A_rel1)\n",
    "\n",
    "# Adjacency matrix of relation number 2\n",
    "A_rel2 = np.random.randint(3,size=(n,n))\n",
    "np.fill_diagonal(A_rel2, 0)  # Not consider the self loop (diag values = 0)\n",
    "A_rel2[A_rel2>1] = 0\n",
    "print('\\n\\n----- Adjacency matrix of relation 2:')\n",
    "print(A_rel2)\n",
    "\n",
    "# Tensor including both adjacency matrices\n",
    "A_rels = np.concatenate((A_rel1, A_rel2))\n",
    "A_rels = np.reshape(A_rels, (num_rels, n, n)) # num_rels is the number of the relations, (n,n) is the dimension of the original adj matrix\n",
    "print('\\n\\n----- Tensor including both adjacency matrices:')\n",
    "print(A_rels)\n",
    "\n",
    "# GCN for each typed edge\n",
    "ND_GCN = np.matmul(A_rels, L_0_rels)\n",
    "print('\\n\\n----- (GCN) Output of the neighborhood diffusion (for each typed edge):')\n",
    "print(ND_GCN)\n",
    "\n",
    "# R-GCN\n",
    "RGCN = np.sum(ND_GCN, axis=0)\n",
    "print('\\n\\n----- (R-GCN) Aggregation of the results of the GCN layer applied to different types of edge:')\n",
    "print(RGCN)\n",
    "\n",
    "# Test of the aggregation\n",
    "assert(RGCN[0,0] == L_0_rels[0,1,0] + L_0_rels[0,2,0] + L_0_rels[0,3,0] + L_0_rels[0,4,0] + L_0_rels[1,3,0])\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ]
}