{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "566c0a97317f6f88d4bc5f478002f1c75c862f0281a52c0ded6c5ead36971532"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Graph Attention Networks (GATs)\n",
    "Original paper: Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). [Graph attention networks](https://arxiv.org/abs/1710.10903). *arXiv preprint arXiv:1710.10903*. \n",
    "\n",
    "## Math Warm-up\n",
    "This section comes from https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html, with minor extensions.\n",
    "\n",
    "### GCN Layer\n",
    "$$h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\frac{1}{c_{ij}} W^{(l)}h^{(l)}_j}\\right)$$\n",
    "\n",
    "* $\\mathcal{N}(i)$: set of the one-hop neighbors (no self-loop) of $n_i$.\n",
    "* $c_{ij}=\\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$: normalization costant based on the graph structure.\n",
    "* $\\sigma$: activation function.\n",
    "* $W^{(l)}$: weight matrix for feature transformation.\n",
    "\n",
    "A broad explanation on Graph Neural Networks (GNNs) is available in the Medium article entitled \"[Understanding the Building Blocks of Graph Neural Networks](https://towardsdatascience.com/understanding-the-building-blocks-of-graph-neural-networks-intro-56627f0719d5)\".\n",
    "\n",
    "### GAT Layer\n",
    "\n",
    "\\begin{split}\\begin{align}\n",
    "z_i^{(l)}&=W^{(l)}h_i^{(l)}, \\\\\n",
    "e_{ij}^{(l)}&=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)})),\\\\\n",
    "\\alpha_{ij}^{(l)}&=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})},\\\\\n",
    "h_i^{(l+1)}&=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right),\n",
    "\\end{align}\\end{split}\n",
    "\n",
    "* Equation (1) is a linear transformation of the lower layer embedding $h_i^{(l)}$ and $W^{(l)}$ is its learnable weight matrix. This transformation is useful to achieve a sufficient expressive power to transform input features (in our example one-hot vectors) into high-level and dense features.\n",
    "* Equation (2) computes a pair-wise *un-normalized* attention score between two neighbors. Here, it first concatenates the $z$ embeddings of the two nodes, where $||$ denotes concatenation, then takes a dot product of it and a learnable weight vector $\\vec a^{(l)}$, and applies a LeakyReLU in the end. This form of attention is usually called additive attention, contrast with the dot-product attention in the Transformer model. The attention score indicates the importance of a neighbor node in the message passing framework.\n",
    "* Equation (3) applies a softmax to normalize the attention scores on each node’s incoming edges.\n",
    "* Equation (4) is similar to GCN. The embeddings from neighbors are aggregated together, scaled by the attention scores.\n",
    "\n",
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "source": [
    "## GAT Layer Implementation with NumPy\n",
    "### Basic Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    return np.where(z > 0, z, z * 0.01)\n",
    "\n",
    "def softmax(z):\n",
    "    if len(z.shape) > 1:\n",
    "        # Softmax for matrix\n",
    "        max_matrix = np.max(z, axis=0)\n",
    "        stable_z = z - max_matrix\n",
    "        e = np.exp(stable_z)\n",
    "        a = e / np.sum(e, axis=0, keepdims=True)\n",
    "    else:\n",
    "        # Softmax for vector\n",
    "        vector_max_value = np.max(z)\n",
    "        a = (np.exp(z - vector_max_value)) / sum(np.exp(z - vector_max_value))\n",
    "\n",
    "    assert a.shape == z.shape\n",
    "\n",
    "    return a\n"
   ]
  },
  {
   "source": [
    "### Graph and Weight Matrix Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- One-hot vector representation of nodes. Shape(n,n)\n\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]]\n\n\n----- Embedding dimension\n\n3\n\n\n----- Weight Matrix. Shape(emb, n)\n\n[[-0.4294049   0.57624235 -0.3047382  -0.11941829 -0.12942953]\n [ 0.19600584  0.5029172   0.3998854  -0.21561317  0.02834577]\n [-0.06529497 -0.31225734  0.03973776  0.47800217 -0.04941563]]\n\n\n----- Adjacency Matrix (undirected graph). Shape(n,n)\n\n[[1 1 1 0 1]\n [1 1 1 1 1]\n [1 1 1 1 0]\n [0 1 1 1 1]\n [1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- One-hot vector representation of nodes. Shape(n,n)\\n')\n",
    "X = np.eye(5, 5)\n",
    "n = X.shape[0]\n",
    "np.random.shuffle(X)\n",
    "print(X)\n",
    "\n",
    "print('\\n\\n----- Embedding dimension\\n')\n",
    "emb = 3\n",
    "print(emb)\n",
    "\n",
    "print('\\n\\n----- Weight Matrix. Shape(emb, n)\\n')\n",
    "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (emb, n))\n",
    "print(W)\n",
    "\n",
    "print('\\n\\n----- Adjacency Matrix (undirected graph). Shape(n,n)\\n')\n",
    "A = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 1)  \n",
    "A = (A + A.T)\n",
    "A[A > 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "source": [
    "### Linear Transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Linear Transformation. Shape(n, emb)\n\n[[-0.3047382   0.3998854   0.03973776]\n [ 0.57624235  0.5029172  -0.31225734]\n [-0.12942953  0.02834577 -0.04941563]\n [-0.4294049   0.19600584 -0.06529497]\n [-0.11941829 -0.21561317  0.47800217]]\n"
     ]
    }
   ],
   "source": [
    "# equation (1)\n",
    "print('\\n\\n----- Linear Transformation. Shape(n, emb)\\n')\n",
    "z1 = X.dot(W.T)\n",
    "print(z1)"
   ]
  },
  {
   "source": [
    "### Transformer: Additive Attention Mechanism"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\n\n[[-0.3047382   0.3998854   0.03973776 -0.3047382   0.3998854   0.03973776]\n [-0.3047382   0.3998854   0.03973776  0.57624235  0.5029172  -0.31225734]\n [-0.3047382   0.3998854   0.03973776 -0.12942953  0.02834577 -0.04941563]\n [-0.3047382   0.3998854   0.03973776 -0.11941829 -0.21561317  0.47800217]\n [ 0.57624235  0.5029172  -0.31225734 -0.3047382   0.3998854   0.03973776]\n [ 0.57624235  0.5029172  -0.31225734  0.57624235  0.5029172  -0.31225734]\n [ 0.57624235  0.5029172  -0.31225734 -0.12942953  0.02834577 -0.04941563]\n [ 0.57624235  0.5029172  -0.31225734 -0.4294049   0.19600584 -0.06529497]\n [ 0.57624235  0.5029172  -0.31225734 -0.11941829 -0.21561317  0.47800217]\n [-0.12942953  0.02834577 -0.04941563 -0.3047382   0.3998854   0.03973776]\n [-0.12942953  0.02834577 -0.04941563  0.57624235  0.5029172  -0.31225734]\n [-0.12942953  0.02834577 -0.04941563 -0.12942953  0.02834577 -0.04941563]\n [-0.12942953  0.02834577 -0.04941563 -0.4294049   0.19600584 -0.06529497]\n [-0.4294049   0.19600584 -0.06529497  0.57624235  0.5029172  -0.31225734]\n [-0.4294049   0.19600584 -0.06529497 -0.12942953  0.02834577 -0.04941563]\n [-0.4294049   0.19600584 -0.06529497 -0.4294049   0.19600584 -0.06529497]\n [-0.4294049   0.19600584 -0.06529497 -0.11941829 -0.21561317  0.47800217]\n [-0.11941829 -0.21561317  0.47800217 -0.3047382   0.3998854   0.03973776]\n [-0.11941829 -0.21561317  0.47800217  0.57624235  0.5029172  -0.31225734]\n [-0.11941829 -0.21561317  0.47800217 -0.4294049   0.19600584 -0.06529497]\n [-0.11941829 -0.21561317  0.47800217 -0.11941829 -0.21561317  0.47800217]]\n\n\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\n\n[[0.09834683 0.42110763 0.95788953 0.53316528 0.69187711 0.31551563]]\n\n\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\n\n[[ 0.30322275]\n [ 0.73315639]\n [ 0.11150219]\n [ 0.11445879]\n [ 0.09607946]\n [ 0.52601309]\n [-0.0956411 ]\n [-0.14458757]\n [-0.0926845 ]\n [ 0.07860653]\n [ 0.50854017]\n [-0.11311402]\n [-0.16206049]\n [ 0.53443082]\n [-0.08722337]\n [-0.13616985]\n [-0.08426678]\n [ 0.48206613]\n [ 0.91199976]\n [ 0.2413991 ]\n [ 0.29330217]]\n\n\n----- Leaky Relu. Shape(1, number of edges)\n[[ 3.03222751e-01]\n [ 7.33156386e-01]\n [ 1.11502195e-01]\n [ 1.14458791e-01]\n [ 9.60794571e-02]\n [ 5.26013092e-01]\n [-9.56410988e-04]\n [-1.44587571e-03]\n [-9.26845030e-04]\n [ 7.86065337e-02]\n [ 5.08540169e-01]\n [-1.13114022e-03]\n [-1.62060495e-03]\n [ 5.34430817e-01]\n [-8.72233739e-04]\n [-1.36169846e-03]\n [-8.42667781e-04]\n [ 4.82066128e-01]\n [ 9.11999763e-01]\n [ 2.41399100e-01]\n [ 2.93302168e-01]]\n"
     ]
    }
   ],
   "source": [
    "# equation (2)\n",
    "print('\\n\\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\\n')\n",
    "edge_coords = np.where(A==1)\n",
    "h_src_nodes = z1[edge_coords[0]]\n",
    "h_dst_nodes = z1[edge_coords[1]]\n",
    "z2 = np.concatenate((h_src_nodes, h_dst_nodes), axis=1)\n",
    "\n",
    "# Concatenation tests\n",
    "assert len(edge_coords[1]) == z2.shape[0], \"The number of edges in A is not equal to the number of concat edges\"\n",
    "test_value = np.array([-0.11941829, -0.12942953, 0.19600584, 0.5029172, 0.3998854, -0.21561317])\n",
    "assert z2[4 ,:].tolist().sort()  == test_value.tolist().sort(), \"Something went wrong in the concat process\"\n",
    "print(z2)\n",
    "\n",
    "print('\\n\\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\\n')\n",
    "att = np.random.rand(1, z2.shape[1])\n",
    "print(att)\n",
    "\n",
    "print('\\n\\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\\n')\n",
    "z2_att = z2.dot(att.T)\n",
    "print(z2_att)\n",
    "\n",
    "print('\\n\\n----- Leaky Relu. Shape(1, number of edges)')\n",
    "e = leaky_relu(z2_att)\n",
    "print(e)"
   ]
  },
  {
   "source": [
    "### Normalize the Attention Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Edge scores as matrix. Shape(n,n)\n\n[[ 3.03222751e-01  7.33156386e-01  1.11502195e-01  0.00000000e+00\n   1.14458791e-01]\n [ 9.60794571e-02  5.26013092e-01 -9.56410988e-04 -1.44587571e-03\n  -9.26845030e-04]\n [ 7.86065337e-02  5.08540169e-01 -1.13114022e-03 -1.62060495e-03\n   0.00000000e+00]\n [ 0.00000000e+00  5.34430817e-01 -8.72233739e-04 -1.36169846e-03\n  -8.42667781e-04]\n [ 4.82066128e-01  9.11999763e-01  0.00000000e+00  2.41399100e-01\n   2.93302168e-01]]\n\n\n----- For each node, normalize the edge (or neighbor) contributions using softmax\n\n[0.26263543 0.21349717 0.20979916 0.31406823 0.21610715 0.17567419\n 0.1726313  0.1771592  0.25842816 0.27167844 0.24278118 0.24273876\n 0.24280162 0.23393014 0.23388927 0.23394984 0.29823075 0.25138555\n 0.22399017 0.22400903 0.30061525]\n\n\n----- Normalized edge score matrix. Shape(n,n)\n\n[[0.26263543 0.21349717 0.20979916 0.         0.31406823]\n [0.21610715 0.17567419 0.1726313  0.1771592  0.25842816]\n [0.27167844 0.24278118 0.24273876 0.24280162 0.        ]\n [0.         0.23393014 0.23388927 0.23394984 0.29823075]\n [0.25138555 0.22399017 0.         0.22400903 0.30061525]]\n"
     ]
    }
   ],
   "source": [
    "# equation (3)\n",
    "print('\\n\\n----- Edge scores as matrix. Shape(n,n)\\n')\n",
    "e_matr = np.zeros(A.shape)\n",
    "e_matr[edge_coords[0], edge_coords[1]] = e.reshape(-1,)\n",
    "print(e_matr)\n",
    "\n",
    "print('\\n\\n----- For each node, normalize the edge (or neighbor) contributions using softmax\\n')\n",
    "alpha0 = softmax(e_matr[:,0][e_matr[:,0] != 0]) \n",
    "alpha1 = softmax(e_matr[:,1][e_matr[:,1] != 0])\n",
    "alpha2 = softmax(e_matr[:,2][e_matr[:,2] != 0])\n",
    "alpha3 = softmax(e_matr[:,3][e_matr[:,3] != 0])\n",
    "alpha4 = softmax(e_matr[:,4][e_matr[:,4] != 0])\n",
    "alpha = np.concatenate((alpha0, alpha1, alpha2, alpha3, alpha4))\n",
    "print(alpha)\n",
    "\n",
    "print('\\n\\n----- Normalized edge score matrix. Shape(n,n)\\n')\n",
    "A_scaled = np.zeros(A.shape)\n",
    "A_scaled[edge_coords[0], edge_coords[1]] = alpha.reshape(-1,)\n",
    "print(A_scaled)"
   ]
  },
  {
   "source": [
    "### Neighborhood Diffusion (GCN) Scaled by the Attention Scores (GAT)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\n\n[[-0.02166863  0.15062515  0.08352843]\n [-0.09390287  0.15866476  0.05716299]\n [-0.07856777  0.28521023 -0.09286313]\n [-0.03154513  0.10583032  0.04267501]\n [-0.07962369  0.19226439  0.069115  ]]\n"
     ]
    }
   ],
   "source": [
    "# equation (4)\n",
    "print('\\n\\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\\n')\n",
    "ND_GAT = A_scaled.dot(z1)\n",
    "print(ND_GAT)"
   ]
  },
  {
   "source": [
    "## GAT Layer - DGL Test\n",
    "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class GATTestLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATTestLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinizialitation modified for testing\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        self.fc.state_dict()['weight'][:] = torch.from_numpy(W)\n",
    "        self.attn_fc.state_dict()['weight'][:] = torch.from_numpy(att)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Create a new DGL graph using the NumPy graph\n\nGraph(num_nodes=5, num_edges=21,\n      ndata_schemes={}\n      edata_schemes={})\n\n\n----- Create a DGL instance of the GAT test layer\n\ntensor([[-0.0217,  0.1506,  0.0835],\n        [-0.0939,  0.1587,  0.0572],\n        [-0.0786,  0.2852, -0.0929],\n        [-0.0315,  0.1058,  0.0427],\n        [-0.0796,  0.1923,  0.0691]], grad_fn=<IndexCopyBackward>)\n\n\n----- Recap of the NumPy GAT layer\n[[-0.0217  0.1506  0.0835]\n [-0.0939  0.1587  0.0572]\n [-0.0786  0.2852 -0.0929]\n [-0.0315  0.1058  0.0427]\n [-0.0796  0.1923  0.0691]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- Create a new DGL graph using the NumPy graph\\n')\n",
    "src_ids = torch.tensor(edge_coords[0])\n",
    "dst_ids = torch.tensor(edge_coords[1])\n",
    "g = dgl.graph((src_ids, dst_ids))\n",
    "print(g)\n",
    "\n",
    "print('\\n\\n----- Create a DGL instance of the GAT test layer\\n')\n",
    "net = GATTestLayer(g,\n",
    "          in_dim=n,\n",
    "          out_dim=3)\n",
    "print(net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n\\n----- Recap of the NumPy GAT layer')\n",
    "print(np.round(ND_GAT, decimals=4))\n",
    "\n"
   ]
  },
  {
   "source": [
    "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Math Warm-up on Multi-head Attention\n",
    "The multi-head attention is useful to enrich the model capability and to stabilize the learning process. The outputs of each attention head can be combined in two different ways:\n",
    "\n",
    "$\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$\n",
    "\n",
    "or\n",
    "\n",
    "$\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$\n",
    "\n",
    "* K is the number of heads. Concatenation is adopted for intermediary layers. The average is employed for the final (prediction) layer, because the concatenation is no longer sensible.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multi Head GAT Layer Implementation with NumPy\n",
    "Multiple head attentions are created generating multiple GAT layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Recap on the output of the GAT layer\n\nLayer 1. Shape(emb,n)\n[[-0.02166863  0.15062515  0.08352843]\n [-0.09390287  0.15866476  0.05716299]\n [-0.07856777  0.28521023 -0.09286313]\n [-0.03154513  0.10583032  0.04267501]\n [-0.07962369  0.19226439  0.069115  ]]\n\nLayer 2. Shape(emb,n)\n[[-0.02166863  0.15062515  0.08352843]\n [-0.09390287  0.15866476  0.05716299]\n [-0.07856777  0.28521023 -0.09286313]\n [-0.03154513  0.10583032  0.04267501]\n [-0.07962369  0.19226439  0.069115  ]]\n\n\n----- Concatenate multiple attentions. Shape(num_layers*emb, n)\n\n[[-0.02166863  0.15062515  0.08352843 -0.02166863  0.15062515  0.08352843]\n [-0.09390287  0.15866476  0.05716299 -0.09390287  0.15866476  0.05716299]\n [-0.07856777  0.28521023 -0.09286313 -0.07856777  0.28521023 -0.09286313]\n [-0.03154513  0.10583032  0.04267501 -0.03154513  0.10583032  0.04267501]\n [-0.07962369  0.19226439  0.069115   -0.07962369  0.19226439  0.069115  ]]\n\n\n----- Average multiple attentions.\n\n0.04979367027023359\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- Recap on the output of the GAT layer')\n",
    "print('\\nLayer 1. Shape(emb,n)')\n",
    "layer1 = ND_GAT\n",
    "print(layer1)\n",
    "\n",
    "print('\\nLayer 2. Shape(emb,n)')\n",
    "layer2 = ND_GAT\n",
    "print(layer2)\n",
    "\n",
    "print('\\n\\n----- Concatenate multiple attentions. Shape(num_layers*emb, n)\\n')\n",
    "concat = np.concatenate((layer1, layer2), axis=1)\n",
    "print(concat)\n",
    "\n",
    "print('\\n\\n----- Average multiple attentions.\\n')\n",
    "# 30 is the number of parameters: num_layers*emb*n\n",
    "average = np.sum((layer1, layer2)) / 30\n",
    "print(average)"
   ]
  },
  {
   "source": [
    "## Multi Head GAT Layer - DGL Test\n",
    "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class MultiHeadGATTestLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATTestLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            # Use the test layer for consistency with the NumPy implementation\n",
    "            self.heads.append(GATTestLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))\n",
    "\n",
    "print('\\n\\n----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\\n')\n",
    "concat_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2)\n",
    "print(concat_net)\n",
    "print('\\n----- DGL concat output\\n')\n",
    "print(concat_net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n----- Recap of the NumPy concatenation\\n')\n",
    "print(np.round(concat, decimals=4))\n",
    "\n",
    "print('\\n\\n----- Multi head GAT Layer (average operation). Shape(emb, n)\\n')\n",
    "mean_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2, merge='mean')\n",
    "print(mean_net)\n",
    "print('\\n----- DGL average output\\n')\n",
    "print(mean_net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n----- Recap of the NumPy average\\n')\n",
    "print(np.round(average, decimals=4))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\n\nMultiHeadGATTestLayer(\n  (heads): ModuleList(\n    (0): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n    (1): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n  )\n)\n\n----- DGL concat output\n\ntensor([[-0.0217,  0.1506,  0.0835, -0.0217,  0.1506,  0.0835],\n        [-0.0939,  0.1587,  0.0572, -0.0939,  0.1587,  0.0572],\n        [-0.0786,  0.2852, -0.0929, -0.0786,  0.2852, -0.0929],\n        [-0.0315,  0.1058,  0.0427, -0.0315,  0.1058,  0.0427],\n        [-0.0796,  0.1923,  0.0691, -0.0796,  0.1923,  0.0691]],\n       grad_fn=<CatBackward>)\n\n----- Recap of the NumPy concatenation\n\n[[-0.0217  0.1506  0.0835 -0.0217  0.1506  0.0835]\n [-0.0939  0.1587  0.0572 -0.0939  0.1587  0.0572]\n [-0.0786  0.2852 -0.0929 -0.0786  0.2852 -0.0929]\n [-0.0315  0.1058  0.0427 -0.0315  0.1058  0.0427]\n [-0.0796  0.1923  0.0691 -0.0796  0.1923  0.0691]]\n\n\n----- Multi head GAT Layer (average operation). Shape(emb, n)\n\nMultiHeadGATTestLayer(\n  (heads): ModuleList(\n    (0): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n    (1): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n  )\n)\n\n----- DGL average output\n\ntensor(0.0498, grad_fn=<MeanBackward0>)\n\n----- Recap of the NumPy average\n\n0.0498\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}