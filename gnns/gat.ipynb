{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit ('learning': conda)",
   "metadata": {
    "interpreter": {
     "hash": "566c0a97317f6f88d4bc5f478002f1c75c862f0281a52c0ded6c5ead36971532"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Graph Attention Networks (GATs)\n",
    "Original paper: Veličković, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., & Bengio, Y. (2017). [Graph attention networks](https://arxiv.org/abs/1710.10903). *arXiv preprint arXiv:1710.10903*. \n",
    "\n",
    "## Math Warm-up\n",
    "This section comes from https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html, with minor extensions.\n",
    "\n",
    "### GCN Layer\n",
    "$$h_i^{(l+1)}=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\frac{1}{c_{ij}} W^{(l)}h^{(l)}_j}\\right)$$\n",
    "\n",
    "* $\\mathcal{N}(i)$: set of the one-hop neighbors (no self-loop) of $n_i$.\n",
    "* $c_{ij}=\\sqrt{|\\mathcal{N}(i)|}\\sqrt{|\\mathcal{N}(j)|}$: normalization costant based on the graph structure.\n",
    "* $\\sigma$: activation function.\n",
    "* $W^{(l)}$: weight matrix for feature transformation.\n",
    "\n",
    "A broad explanation on Graph Neural Networks (GNNs) is available in the Medium article entitled \"[Understanding the Building Blocks of Graph Neural Networks](https://towardsdatascience.com/understanding-the-building-blocks-of-graph-neural-networks-intro-56627f0719d5)\".\n",
    "\n",
    "### GAT Layer\n",
    "\n",
    "\\begin{split}\\begin{align}\n",
    "z_i^{(l)}&=W^{(l)}h_i^{(l)}, \\\\\n",
    "e_{ij}^{(l)}&=\\text{LeakyReLU}(\\vec a^{(l)^T}(z_i^{(l)}||z_j^{(l)})),\\\\\n",
    "\\alpha_{ij}^{(l)}&=\\frac{\\exp(e_{ij}^{(l)})}{\\sum_{k\\in \\mathcal{N}(i)}^{}\\exp(e_{ik}^{(l)})},\\\\\n",
    "h_i^{(l+1)}&=\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)} {\\alpha^{(l)}_{ij} z^{(l)}_j }\\right),\n",
    "\\end{align}\\end{split}\n",
    "\n",
    "* Equation (1) is a linear transformation of the lower layer embedding $h_i^{(l)}$ and $W^{(l)}$ is its learnable weight matrix. This transformation is useful to achieve a sufficient expressive power to transform input features (in our example one-hot vectors) into high-level and dense features.\n",
    "* Equation (2) computes a pair-wise *un-normalized* attention score between two neighbors. Here, it first concatenates the $z$ embeddings of the two nodes, where $||$ denotes concatenation, then takes a dot product of it and a learnable weight vector $\\vec a^{(l)}$, and applies a LeakyReLU in the end. This form of attention is usually called additive attention, contrast with the dot-product attention in the Transformer model. The attention score indicates the importance of a neighbor node in the message passing framework.\n",
    "* Equation (3) applies a softmax to normalize the attention scores on each node’s incoming edges.\n",
    "* Equation (4) is similar to GCN. The embeddings from neighbors are aggregated together, scaled by the attention scores.\n",
    "\n",
    "## Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "\n",
    "np.random.seed(1)\n"
   ]
  },
  {
   "source": [
    "## GAT Layer Implementation with NumPy\n",
    "### Basic Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z):\n",
    "    return np.where(z > 0, z, z * 0.01)\n",
    "\n",
    "def softmax(z):\n",
    "    if len(z.shape) > 1:\n",
    "        # Softmax for matrix\n",
    "        max_matrix = np.max(z, axis=0)\n",
    "        stable_z = z - max_matrix\n",
    "        e = np.exp(stable_z)\n",
    "        a = e / np.sum(e, axis=0, keepdims=True)\n",
    "    else:\n",
    "        # Softmax for vector\n",
    "        vector_max_value = np.max(z)\n",
    "        a = (np.exp(z - vector_max_value)) / sum(np.exp(z - vector_max_value))\n",
    "\n",
    "    assert a.shape == z.shape\n",
    "\n",
    "    return a\n"
   ]
  },
  {
   "source": [
    "### Graph and Weight Matrix Generation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- One-hot vector representation of nodes. Shape(n,n)\n\n[[0. 0. 1. 0. 0.]\n [0. 1. 0. 0. 0.]\n [0. 0. 0. 0. 1.]\n [1. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0.]]\n\n\n----- Embedding dimension\n\n3\n\n\n----- Weight Matrix. Shape(emb, n)\n\n[[-0.4294049   0.57624235 -0.3047382  -0.11941829 -0.12942953]\n [ 0.19600584  0.5029172   0.3998854  -0.21561317  0.02834577]\n [-0.06529497 -0.31225734  0.03973776  0.47800217 -0.04941563]]\n\n\n----- Adjacency Matrix (undirected graph). Shape(n,n)\n\n[[1 1 1 0 1]\n [1 1 1 1 1]\n [1 1 1 1 0]\n [0 1 1 1 1]\n [1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- One-hot vector representation of nodes. Shape(n,n)\\n')\n",
    "X = np.eye(5, 5)\n",
    "n = X.shape[0]\n",
    "np.random.shuffle(X)\n",
    "print(X)\n",
    "\n",
    "print('\\n\\n----- Embedding dimension\\n')\n",
    "emb = 3\n",
    "print(emb)\n",
    "\n",
    "print('\\n\\n----- Weight Matrix. Shape(emb, n)\\n')\n",
    "W = np.random.uniform(-np.sqrt(1. / emb), np.sqrt(1. / emb), (emb, n))\n",
    "print(W)\n",
    "\n",
    "print('\\n\\n----- Adjacency Matrix (undirected graph). Shape(n,n)\\n')\n",
    "A = np.random.randint(2, size=(n, n))\n",
    "np.fill_diagonal(A, 1)  \n",
    "A = (A + A.T)\n",
    "A[A > 1] = 1\n",
    "print(A)"
   ]
  },
  {
   "source": [
    "### Linear Transformation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Linear Transformation. Shape(n, emb)\n\n[[-0.3047382   0.3998854   0.03973776]\n [ 0.57624235  0.5029172  -0.31225734]\n [-0.12942953  0.02834577 -0.04941563]\n [-0.4294049   0.19600584 -0.06529497]\n [-0.11941829 -0.21561317  0.47800217]]\n"
     ]
    }
   ],
   "source": [
    "# equation (1)\n",
    "print('\\n\\n----- Linear Transformation. Shape(n, emb)\\n')\n",
    "z1 = X.dot(W.T)\n",
    "print(z1)"
   ]
  },
  {
   "source": [
    "### Transformer: Additive Attention Mechanism"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\n\n[[-0.3047382   0.3998854   0.03973776 -0.3047382   0.3998854   0.03973776]\n [-0.3047382   0.3998854   0.03973776  0.57624235  0.5029172  -0.31225734]\n [-0.3047382   0.3998854   0.03973776 -0.12942953  0.02834577 -0.04941563]\n [-0.3047382   0.3998854   0.03973776 -0.11941829 -0.21561317  0.47800217]\n [ 0.57624235  0.5029172  -0.31225734 -0.3047382   0.3998854   0.03973776]\n [ 0.57624235  0.5029172  -0.31225734  0.57624235  0.5029172  -0.31225734]\n [ 0.57624235  0.5029172  -0.31225734 -0.12942953  0.02834577 -0.04941563]\n [ 0.57624235  0.5029172  -0.31225734 -0.4294049   0.19600584 -0.06529497]\n [ 0.57624235  0.5029172  -0.31225734 -0.11941829 -0.21561317  0.47800217]\n [-0.12942953  0.02834577 -0.04941563 -0.3047382   0.3998854   0.03973776]\n [-0.12942953  0.02834577 -0.04941563  0.57624235  0.5029172  -0.31225734]\n [-0.12942953  0.02834577 -0.04941563 -0.12942953  0.02834577 -0.04941563]\n [-0.12942953  0.02834577 -0.04941563 -0.4294049   0.19600584 -0.06529497]\n [-0.4294049   0.19600584 -0.06529497  0.57624235  0.5029172  -0.31225734]\n [-0.4294049   0.19600584 -0.06529497 -0.12942953  0.02834577 -0.04941563]\n [-0.4294049   0.19600584 -0.06529497 -0.4294049   0.19600584 -0.06529497]\n [-0.4294049   0.19600584 -0.06529497 -0.11941829 -0.21561317  0.47800217]\n [-0.11941829 -0.21561317  0.47800217 -0.3047382   0.3998854   0.03973776]\n [-0.11941829 -0.21561317  0.47800217  0.57624235  0.5029172  -0.31225734]\n [-0.11941829 -0.21561317  0.47800217 -0.4294049   0.19600584 -0.06529497]\n [-0.11941829 -0.21561317  0.47800217 -0.11941829 -0.21561317  0.47800217]]\n\n\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\n\n[[0.09834683 0.42110763 0.95788953 0.53316528 0.69187711 0.31551563]]\n\n\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\n\n[[ 0.30322275]\n [ 0.73315639]\n [ 0.11150219]\n [ 0.11445879]\n [ 0.09607946]\n [ 0.52601309]\n [-0.0956411 ]\n [-0.14458757]\n [-0.0926845 ]\n [ 0.07860653]\n [ 0.50854017]\n [-0.11311402]\n [-0.16206049]\n [ 0.53443082]\n [-0.08722337]\n [-0.13616985]\n [-0.08426678]\n [ 0.48206613]\n [ 0.91199976]\n [ 0.2413991 ]\n [ 0.29330217]]\n\n\n----- Leaky Relu. Shape(1, number of edges)\n[[ 3.03222751e-01]\n [ 7.33156386e-01]\n [ 1.11502195e-01]\n [ 1.14458791e-01]\n [ 9.60794571e-02]\n [ 5.26013092e-01]\n [-9.56410988e-04]\n [-1.44587571e-03]\n [-9.26845030e-04]\n [ 7.86065337e-02]\n [ 5.08540169e-01]\n [-1.13114022e-03]\n [-1.62060495e-03]\n [ 5.34430817e-01]\n [-8.72233739e-04]\n [-1.36169846e-03]\n [-8.42667781e-04]\n [ 4.82066128e-01]\n [ 9.11999763e-01]\n [ 2.41399100e-01]\n [ 2.93302168e-01]]\n"
     ]
    }
   ],
   "source": [
    "# equation (2)\n",
    "print('\\n\\n----- Concat hidden features to represent edges. Shape(len(emb.concat(emb)), number of edges)\\n')\n",
    "edge_coords = np.where(A==1)\n",
    "h_src_nodes = z1[edge_coords[0]]\n",
    "h_dst_nodes = z1[edge_coords[1]]\n",
    "z2 = np.concatenate((h_src_nodes, h_dst_nodes), axis=1)\n",
    "\n",
    "# Concatenation tests\n",
    "assert len(edge_coords[1]) == z2.shape[0], \"The number of edges in A is not equal to the number of concat edges\"\n",
    "test_value = np.array([-0.11941829, -0.12942953, 0.19600584, 0.5029172, 0.3998854, -0.21561317])\n",
    "assert z2[4 ,:].tolist().sort()  == test_value.tolist().sort(), \"Something went wrong in the concat process\"\n",
    "print(z2)\n",
    "\n",
    "print('\\n\\n----- Attention coefficients. Shape(1, len(emb.concat(emb)))\\n')\n",
    "att = np.random.rand(1, z2.shape[1])\n",
    "print(att)\n",
    "\n",
    "print('\\n\\n----- Edge representations combined with the attention coefficients. Shape(1, number of edges)\\n')\n",
    "z2_att = z2.dot(att.T)\n",
    "print(z2_att)\n",
    "\n",
    "print('\\n\\n----- Leaky Relu. Shape(1, number of edges)')\n",
    "e = leaky_relu(z2_att)\n",
    "print(e)"
   ]
  },
  {
   "source": [
    "### Normalize the Attention Scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Edge scores as matrix. Shape(n,n)\n\n[[ 3.03222751e-01  7.33156386e-01  1.11502195e-01  0.00000000e+00\n   1.14458791e-01]\n [ 9.60794571e-02  5.26013092e-01 -9.56410988e-04 -1.44587571e-03\n  -9.26845030e-04]\n [ 7.86065337e-02  5.08540169e-01 -1.13114022e-03 -1.62060495e-03\n   0.00000000e+00]\n [ 0.00000000e+00  5.34430817e-01 -8.72233739e-04 -1.36169846e-03\n  -8.42667781e-04]\n [ 4.82066128e-01  9.11999763e-01  0.00000000e+00  2.41399100e-01\n   2.93302168e-01]]\n\n\n----- For each node, normalize the edge (or neighbor) contributions using softmax\n\n[0.26263543 0.21349717 0.20979916 0.31406823 0.21610715 0.17567419\n 0.1726313  0.1771592  0.25842816 0.27167844 0.24278118 0.24273876\n 0.24280162 0.23393014 0.23388927 0.23394984 0.29823075 0.25138555\n 0.22399017 0.22400903 0.30061525]\n\n\n----- Normalized edge score matrix. Shape(n,n)\n\n[[0.26263543 0.21349717 0.20979916 0.         0.31406823]\n [0.21610715 0.17567419 0.1726313  0.1771592  0.25842816]\n [0.27167844 0.24278118 0.24273876 0.24280162 0.        ]\n [0.         0.23393014 0.23388927 0.23394984 0.29823075]\n [0.25138555 0.22399017 0.         0.22400903 0.30061525]]\n"
     ]
    }
   ],
   "source": [
    "# equation (3)\n",
    "print('\\n\\n----- Edge scores as matrix. Shape(n,n)\\n')\n",
    "e_matr = np.zeros(A.shape)\n",
    "e_matr[edge_coords[0], edge_coords[1]] = e.reshape(-1,)\n",
    "print(e_matr)\n",
    "\n",
    "print('\\n\\n----- For each node, normalize the edge (or neighbor) contributions using softmax\\n')\n",
    "alpha0 = softmax(e_matr[:,0][e_matr[:,0] != 0]) \n",
    "alpha1 = softmax(e_matr[:,1][e_matr[:,1] != 0])\n",
    "alpha2 = softmax(e_matr[:,2][e_matr[:,2] != 0])\n",
    "alpha3 = softmax(e_matr[:,3][e_matr[:,3] != 0])\n",
    "alpha4 = softmax(e_matr[:,4][e_matr[:,4] != 0])\n",
    "alpha = np.concatenate((alpha0, alpha1, alpha2, alpha3, alpha4))\n",
    "print(alpha)\n",
    "\n",
    "print('\\n\\n----- Normalized edge score matrix. Shape(n,n)\\n')\n",
    "A_scaled = np.zeros(A.shape)\n",
    "A_scaled[edge_coords[0], edge_coords[1]] = alpha.reshape(-1,)\n",
    "print(A_scaled)"
   ]
  },
  {
   "source": [
    "### Neighborhood Diffusion (GCN) Scaled by the Attention Scores (GAT)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\n\n[[-0.02166863  0.15062515  0.08352843]\n [-0.09390287  0.15866476  0.05716299]\n [-0.07856777  0.28521023 -0.09286313]\n [-0.03154513  0.10583032  0.04267501]\n [-0.07962369  0.19226439  0.069115  ]]\n"
     ]
    }
   ],
   "source": [
    "# equation (4)\n",
    "print('\\n\\nNeighborhood aggregation (GCN) scaled with attention scores (GAT). Shape(n, emb)\\n')\n",
    "ND_GAT = A_scaled.dot(z1)\n",
    "print(ND_GAT)"
   ]
  },
  {
   "source": [
    "## GAT Layer - DGL Test\n",
    "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class GATTestLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATTestLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinizialitation modified for testing\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        self.fc.state_dict()['weight'][:] = torch.from_numpy(W)\n",
    "        self.attn_fc.state_dict()['weight'][:] = torch.from_numpy(att)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Create a new DGL graph using the NumPy graph\n\nGraph(num_nodes=5, num_edges=21,\n      ndata_schemes={}\n      edata_schemes={})\n\n\n----- Create a DGL instance of the GAT test layer\n\ntensor([[-0.0217,  0.1506,  0.0835],\n        [-0.0939,  0.1587,  0.0572],\n        [-0.0786,  0.2852, -0.0929],\n        [-0.0315,  0.1058,  0.0427],\n        [-0.0796,  0.1923,  0.0691]], grad_fn=<IndexCopyBackward>)\n\n\n----- Recap of the NumPy GAT layer\n[[-0.0217  0.1506  0.0835]\n [-0.0939  0.1587  0.0572]\n [-0.0786  0.2852 -0.0929]\n [-0.0315  0.1058  0.0427]\n [-0.0796  0.1923  0.0691]]\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- Create a new DGL graph using the NumPy graph\\n')\n",
    "src_ids = torch.tensor(edge_coords[0])\n",
    "dst_ids = torch.tensor(edge_coords[1])\n",
    "g = dgl.graph((src_ids, dst_ids))\n",
    "print(g)\n",
    "\n",
    "print('\\n\\n----- Create a DGL instance of the GAT test layer\\n')\n",
    "net = GATTestLayer(g,\n",
    "          in_dim=n,\n",
    "          out_dim=3)\n",
    "print(net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n\\n----- Recap of the NumPy GAT layer')\n",
    "print(np.round(ND_GAT, decimals=4))\n",
    "\n"
   ]
  },
  {
   "source": [
    "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Math Warm-up on Multi-head Attention\n",
    "The multi-head attention is useful to enrich the model capability and to stabilize the learning process. The outputs of each attention head can be combined in two different ways:\n",
    "\n",
    "$\\text{concatenation}: h^{(l+1)}_{i} =||_{k=1}^{K}\\sigma\\left(\\sum_{j\\in \\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$\n",
    "\n",
    "or\n",
    "\n",
    "$\\text{average}: h_{i}^{(l+1)}=\\sigma\\left(\\frac{1}{K}\\sum_{k=1}^{K}\\sum_{j\\in\\mathcal{N}(i)}\\alpha_{ij}^{k}W^{k}h^{(l)}_{j}\\right)$\n",
    "\n",
    "* K is the number of heads. Concatenation is adopted for intermediary layers. The average is employed for the final (prediction) layer, because the concatenation is no longer sensible.\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Multi Head GAT Layer Implementation with NumPy\n",
    "Multiple head attentions are created generating multiple GAT layers."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "----- Recap on the output of the GAT layer\n",
      "\n",
      "Layer 1. Shape(emb,n)\n",
      "[[-0.02166863  0.15062515  0.08352843]\n",
      " [-0.09390287  0.15866476  0.05716299]\n",
      " [-0.07856777  0.28521023 -0.09286313]\n",
      " [-0.03154513  0.10583032  0.04267501]\n",
      " [-0.07962369  0.19226439  0.069115  ]]\n",
      "\n",
      "Layer 2. Shape(emb,n)\n",
      "[[-0.02166863  0.15062515  0.08352843]\n",
      " [-0.09390287  0.15866476  0.05716299]\n",
      " [-0.07856777  0.28521023 -0.09286313]\n",
      " [-0.03154513  0.10583032  0.04267501]\n",
      " [-0.07962369  0.19226439  0.069115  ]]\n",
      "\n",
      "\n",
      "----- Concatenate multiple attentions. Shape(num_layers*emb, n)\n",
      "\n",
      "[[-0.02166863  0.15062515  0.08352843 -0.02166863  0.15062515  0.08352843]\n",
      " [-0.09390287  0.15866476  0.05716299 -0.09390287  0.15866476  0.05716299]\n",
      " [-0.07856777  0.28521023 -0.09286313 -0.07856777  0.28521023 -0.09286313]\n",
      " [-0.03154513  0.10583032  0.04267501 -0.03154513  0.10583032  0.04267501]\n",
      " [-0.07962369  0.19226439  0.069115   -0.07962369  0.19226439  0.069115  ]]\n",
      "\n",
      "\n",
      "----- Average multiple attentions.\n",
      "\n",
      "0.04979367027023359\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n----- Recap on the output of the GAT layer')\n",
    "print('\\nLayer 1. Shape(emb,n)')\n",
    "layer1 = ND_GAT\n",
    "print(layer1)\n",
    "\n",
    "print('\\nLayer 2. Shape(emb,n)')\n",
    "layer2 = ND_GAT\n",
    "print(layer2)\n",
    "\n",
    "print('\\n\\n----- Concatenate multiple attentions. Shape(num_layers*emb, n)\\n')\n",
    "concat = np.concatenate((layer1, layer2), axis=1)\n",
    "print(concat)\n",
    "\n",
    "print('\\n\\n----- Average multiple attentions.\\n')\n",
    "# 30 is the number of parameters: num_layers*emb*n\n",
    "average = np.sum((layer1, layer2)) / 30\n",
    "print(average)"
   ]
  },
  {
   "source": [
    "## Multi Head GAT Layer - DGL Test\n",
    "Original layer implementation: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "class MultiHeadGATTestLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATTestLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            # Use the test layer for consistency with the NumPy implementation\n",
    "            self.heads.append(GATTestLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))\n",
    "\n",
    "print('\\n\\n----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\\n')\n",
    "concat_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2)\n",
    "print(concat_net)\n",
    "print('\\n----- DGL concat output\\n')\n",
    "print(concat_net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n----- Recap of the NumPy concatenation\\n')\n",
    "print(np.round(concat, decimals=4))\n",
    "\n",
    "print('\\n\\n----- Multi head GAT Layer (average operation). Shape(emb, n)\\n')\n",
    "mean_net = MultiHeadGATTestLayer(g, in_dim=n, out_dim=3, num_heads=2, merge='mean')\n",
    "print(mean_net)\n",
    "print('\\n----- DGL average output\\n')\n",
    "print(mean_net.forward(torch.Tensor(X)))\n",
    "\n",
    "print('\\n----- Recap of the NumPy average\\n')\n",
    "print(np.round(average, decimals=4))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n\n----- Multi head GAT layer (concat operation). Shape(num_layers*emb, n)\n\nMultiHeadGATTestLayer(\n  (heads): ModuleList(\n    (0): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n    (1): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n  )\n)\n\n----- DGL concat output\n\ntensor([[-0.0217,  0.1506,  0.0835, -0.0217,  0.1506,  0.0835],\n        [-0.0939,  0.1587,  0.0572, -0.0939,  0.1587,  0.0572],\n        [-0.0786,  0.2852, -0.0929, -0.0786,  0.2852, -0.0929],\n        [-0.0315,  0.1058,  0.0427, -0.0315,  0.1058,  0.0427],\n        [-0.0796,  0.1923,  0.0691, -0.0796,  0.1923,  0.0691]],\n       grad_fn=<CatBackward>)\n\n----- Recap of the NumPy concatenation\n\n[[-0.0217  0.1506  0.0835 -0.0217  0.1506  0.0835]\n [-0.0939  0.1587  0.0572 -0.0939  0.1587  0.0572]\n [-0.0786  0.2852 -0.0929 -0.0786  0.2852 -0.0929]\n [-0.0315  0.1058  0.0427 -0.0315  0.1058  0.0427]\n [-0.0796  0.1923  0.0691 -0.0796  0.1923  0.0691]]\n\n\n----- Multi head GAT Layer (average operation). Shape(emb, n)\n\nMultiHeadGATTestLayer(\n  (heads): ModuleList(\n    (0): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n    (1): GATTestLayer(\n      (fc): Linear(in_features=5, out_features=3, bias=False)\n      (attn_fc): Linear(in_features=6, out_features=1, bias=False)\n    )\n  )\n)\n\n----- DGL average output\n\ntensor(0.0498, grad_fn=<MeanBackward0>)\n\n----- Recap of the NumPy average\n\n0.0498\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The resulting matrices from the NumPy implementation and the DGL implementation are equal \\o/."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# From Theory to Practice\n",
    "After the understanding of math and the implementation of GAT building blocks, we can run some experiments as reported in the original paper. Let's recap the DGL modules using a fair parameter initialization. The following implementation is based on the example available here: https://docs.dgl.ai/en/0.4.x/tutorials/models/1_gnn/9_gat.html."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## New Imports"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dgl import DGLGraph\n",
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx"
   ]
  },
  {
   "source": [
    "## GAT Implementation with DGL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim):\n",
    "        super(GATLayer, self).__init__()\n",
    "        self.g = g\n",
    "        # equation (1)\n",
    "        self.fc = nn.Linear(in_dim, out_dim, bias=False)\n",
    "        # equation (2)\n",
    "        self.attn_fc = nn.Linear(2 * out_dim, 1, bias=False)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize learnable parameters.\"\"\"\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_fc.weight, gain=gain)\n",
    "\n",
    "    def edge_attention(self, edges):\n",
    "        # edge UDF for equation (2)\n",
    "        z2 = torch.cat([edges.src['z'], edges.dst['z']], dim=1)\n",
    "        a = self.attn_fc(z2)\n",
    "        return {'e': F.leaky_relu(a)}\n",
    "\n",
    "    def message_func(self, edges):\n",
    "        # message UDF for equation (3) & (4)\n",
    "        return {'z': edges.src['z'], 'e': edges.data['e']}\n",
    "\n",
    "    def reduce_func(self, nodes):\n",
    "        # reduce UDF for equation (3) & (4)\n",
    "        # equation (3)\n",
    "        alpha = F.softmax(nodes.mailbox['e'], dim=1)\n",
    "        # equation (4)\n",
    "        h = torch.sum(alpha * nodes.mailbox['z'], dim=1)\n",
    "        return {'h': h}\n",
    "\n",
    "    def forward(self, h):\n",
    "        # equation (1)\n",
    "        z = self.fc(h)\n",
    "        self.g.ndata['z'] = z\n",
    "        # equation (2)\n",
    "        self.g.apply_edges(self.edge_attention)\n",
    "        # equation (3) & (4)\n",
    "        self.g.update_all(self.message_func, self.reduce_func)\n",
    "        return self.g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, g, in_dim, out_dim, num_heads, merge='cat'):\n",
    "        super(MultiHeadGATLayer, self).__init__()\n",
    "        self.heads = nn.ModuleList()\n",
    "        for i in range(num_heads):\n",
    "            self.heads.append(GATLayer(g, in_dim, out_dim))\n",
    "        self.merge = merge\n",
    "\n",
    "    def forward(self, h):\n",
    "        head_outs = [attn_head(h) for attn_head in self.heads]\n",
    "        if self.merge == 'cat':\n",
    "            # concat on the output feature dimension (dim=1)\n",
    "            return torch.cat(head_outs, dim=1)\n",
    "        else:\n",
    "            # merge using average\n",
    "            return torch.mean(torch.stack(head_outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, g, in_dim, hidden_dim, out_dim, num_heads):\n",
    "        super(GAT, self).__init__()\n",
    "        self.layer1 = MultiHeadGATLayer(g, in_dim, hidden_dim, num_heads)\n",
    "        # Be aware that the input dimension is hidden_dim*num_heads since\n",
    "        # multiple head outputs are concatenated together. Also, only\n",
    "        # one attention head in the output layer.\n",
    "        self.layer2 = MultiHeadGATLayer(g, hidden_dim * num_heads, out_dim, 1)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.layer1(h)\n",
    "        h = F.elu(h)\n",
    "        h = self.layer2(h)\n",
    "        return h"
   ]
  },
  {
   "source": [
    "## Evaluation Functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "    _, indices = torch.max(logits, dim=1)\n",
    "    correct = torch.sum(indices == labels)\n",
    "    return correct.item() * 1.0 / len(labels)\n",
    "\n",
    "def evaluate(model, features, labels, mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        return accuracy(logits, labels)"
   ]
  },
  {
   "source": [
    "## Load Cora Dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading from cache failed, re-processing.\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e4339d375069>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_cora_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n----- Features of CORA dataset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-e4339d375069>\u001b[0m in \u001b[0;36mload_cora_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_cora_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcitegrh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_cora\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtrain_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBoolTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/citation_graph.py\u001b[0m in \u001b[0;36mload_cora\u001b[0;34m(raw_dir, force_reload, verbose)\u001b[0m\n\u001b[1;32m    715\u001b[0m     \u001b[0mCoraGraphDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m     \"\"\"\n\u001b[0;32m--> 717\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoraGraphDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/citation_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, raw_dir, force_reload, verbose)\u001b[0m\n\u001b[1;32m    387\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'cora'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 389\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCoraGraphDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/citation_graph.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, raw_dir, force_reload, verbose)\u001b[0m\n\u001b[1;32m     65\u001b[0m                                                    \u001b[0mraw_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mraw_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                                                    \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                                                    verbose=verbose)\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/dgl_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, url, raw_dir, hash_key, force_reload, verbose)\u001b[0m\n\u001b[1;32m    284\u001b[0m                                                 \u001b[0mhash_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhash_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                                                 \u001b[0mforce_reload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_reload\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                                                 verbose=verbose)\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/dgl_dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, url, raw_dir, save_dir, hash_key, force_reload, verbose)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/dgl_dataset.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mload_flag\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/dgl/data/citation_graph.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx_reorder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict_of_lists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_intXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# Everything else takes the normal path.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mIndexMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_asindices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/scipy/sparse/_index.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_arrayXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# row.ndim == 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINT_TYPES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m_get_arrayXslice\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_arrayXslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_row_ranges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_intXarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m_get_row_ranges\u001b[0;34m(self, rows, col_slice)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mcol_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0mnj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlil_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         _csparsetools.lil_get_row_ranges(self.shape[0], self.shape[1],\n",
      "\u001b[0;32m~/anaconda3/envs/learning/lib/python3.6/site-packages/scipy/sparse/lil.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unrecognized lil_matrix constructor usage'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = torch.FloatTensor(data.features)\n",
    "    labels = torch.LongTensor(data.labels)\n",
    "    train_mask = torch.BoolTensor(data.train_mask)\n",
    "    val_mask = torch.BoolTensor(data.val_mask)\n",
    "    test_mask = torch.BoolTensor(data.test_mask)\n",
    "    g = DGLGraph(data.graph)\n",
    "    return g, features, labels, train_mask, val_mask, test_mask\n",
    "\n",
    "g, features, labels, train_mask, val_mask, test_mask = load_cora_data()\n",
    "print('\\n\\n----- Features of CORA dataset')\n",
    "\n",
    "print('\\n----- Graph:')\n",
    "print(g)\n",
    "\n",
    "print('\\n----- Features:')\n",
    "print(features)\n",
    "print(features.nonzero(as_tuple=True)[1])\n",
    "\n",
    "print('\\n----- Labels:')\n",
    "print(labels)\n",
    "print(labels.size())\n",
    "output = torch.unique(labels)\n",
    "occs = torch.bincount(labels)\n",
    "print('----- Number of unique labels:')\n",
    "print(output)\n",
    "print('----- Number of label occurrences:')\n",
    "print(occs)\n",
    "\n",
    "print('\\n----- Training mask:')\n",
    "train_long = train_mask.long()\n",
    "occs = torch.bincount(train_long)\n",
    "print(output)\n",
    "print(occs)\n",
    "\n",
    "print('\\n----- Validation mask:')\n",
    "val_long = val_mask.long()\n",
    "occs = torch.bincount(val_long)\n",
    "print(output)\n",
    "print(occs)\n",
    "\n",
    "print('\\n----- Testing mask:')\n",
    "test_long = test_mask.long()\n",
    "occs = torch.bincount(test_long)\n",
    "print(output)\n",
    "print(occs)\n"
   ]
  },
  {
   "source": [
    "Analyzing the cora dataset, you can get the following information:\n",
    "\n",
    "1. Nodes have no features (one-hot encoding vectors)\n",
    "2. Node labels are uniformly distributed\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Training Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model, 2 heads, each head has hidden size 8\n",
    "model = GAT(g,\n",
    "          in_dim=features.size()[1],\n",
    "          hidden_dim=8,\n",
    "          out_dim=7,\n",
    "          num_heads=2)\n",
    "\n",
    "# create optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# main loop\n",
    "dur = []\n",
    "for epoch in range(300):\n",
    "    t0 = time.time()\n",
    "\n",
    "    logits = model(features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    train_acc = accuracy(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    dur.append(time.time() - t0)\n",
    "\n",
    "    print(\"Epoch {:05d} | Time(s) {:.4f} | Loss {:.4f} | Training Accuracy {:.4f}\".format(\n",
    "        epoch, np.mean(dur), loss.item(), train_acc))\n",
    "\n",
    "    if epoch % 30==0:\n",
    "        print(\"\\nEval on validation dataset...\")\n",
    "        val_acc = evaluate(model, features, labels, val_mask)\n",
    "        print(\"Validation Accuracy: {:.4f}\\n\".format(val_acc))\n",
    "\n",
    "print()\n",
    "acc = evaluate(model, features, labels, test_mask)\n",
    "print(\"Test Accuracy {:.4f}\".format(acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}